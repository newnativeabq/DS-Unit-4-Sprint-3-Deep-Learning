{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "#Words, words, mere words, no matter from the heart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Path to File\n",
    "import os\n",
    "\n",
    "\n",
    "filename = \"shakespeare\"\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 72156: expected 1 fields, saw 2\\nSkipping line 107004: expected 1 fields, saw 2\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From fairest creatures we desire increase,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>That thereby beauty’s rose might never die,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>But as the riper should by time decease,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>His tender heir might bear his memory:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             0\n",
       "0                                            1\n",
       "1   From fairest creatures we desire increase,\n",
       "2  That thereby beauty’s rose might never die,\n",
       "3     But as the riper should by time decease,\n",
       "4       His tender heir might bear his memory:"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read all lines into dataframe for early processing\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_table(\n",
    "    filepath,\n",
    "    header=None,\n",
    "    skip_blank_lines=True,\n",
    "    lineterminator=None,\n",
    "    skiprows=136,\n",
    "    error_bad_lines=False,\n",
    "    verbose=False,\n",
    "           )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From fairest creatures we desire increase,</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>That thereby beauty’s rose might never die,</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>But as the riper should by time decease,</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>His tender heir might bear his memory:</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text  len\n",
       "0                                            1   21\n",
       "1   From fairest creatures we desire increase,   42\n",
       "2  That thereby beauty’s rose might never die,   43\n",
       "3     But as the riper should by time decease,   40\n",
       "4       His tender heir might bear his memory:   38"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate length of each line\n",
    "def get_len(x):\n",
    "    return len(x)\n",
    "\n",
    "df.columns = ['text']\n",
    "\n",
    "df['len'] = df.text.apply(get_len)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.axes._subplots.AxesSubplot at 0x7f4e05454950>,\n",
       "                  len\n",
       " count  138502.000000\n",
       " mean       39.106627\n",
       " std        17.247505\n",
       " min         2.000000\n",
       " 25%        29.000000\n",
       " 50%        42.000000\n",
       " 75%        49.000000\n",
       " max       500.000000)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check distribution of len\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.kdeplot(df.len), df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "len    39.0\n",
       "Name: 0.375, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "len    71.0\n",
       "Name: 0.99, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>From fairest creatures we desire increase,</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>That thereby beauty’s rose might never die,</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>But as the riper should by time decease,</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>His tender heir might bear his memory:</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>But thou contracted to thine own bright eyes,</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text  len\n",
       "1     From fairest creatures we desire increase,   42\n",
       "2    That thereby beauty’s rose might never die,   43\n",
       "3       But as the riper should by time decease,   40\n",
       "4         His tender heir might bear his memory:   38\n",
       "5  But thou contracted to thine own bright eyes,   45"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Truncate data between quantiles\n",
    "# Lower Bound: 0.375\n",
    "# Upper bound: 0.99\n",
    "\n",
    "display(df.quantile(0.375), df.quantile(0.99))\n",
    "df_trunc = df[(df.len > 26) & (df.len < 69)]\n",
    "df_trunc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.axes._subplots.AxesSubplot at 0x7f4e0b317550>,\n",
       "                  len\n",
       " count  101327.000000\n",
       " mean       45.799392\n",
       " std         9.224490\n",
       " min        27.000000\n",
       " 25%        40.000000\n",
       " 50%        45.000000\n",
       " 75%        50.000000\n",
       " max        68.000000)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(df_trunc.len), df_trunc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>138127</td>\n",
       "      <td>Teaching decrepit age to tread the measures;  ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138128</td>\n",
       "      <td>The staring ruffian shall it keep in quiet,</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138129</td>\n",
       "      <td>Pluck down the rich, enrich the poor with trea...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138130</td>\n",
       "      <td>It shall be raging mad, and silly mild,</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138131</td>\n",
       "      <td>Make the young old, the old become a child. ...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138491</td>\n",
       "      <td>Gutenberg-tm eBooks with only a loose network ...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138492</td>\n",
       "      <td>Project Gutenberg-tm eBooks are often created ...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138494</td>\n",
       "      <td>the U.S. unless a copyright notice is included...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138499</td>\n",
       "      <td>how to make donations to the Project Gutenberg...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138501</td>\n",
       "      <td>our email newsletter to hear about new eBooks.</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  len\n",
       "138127  Teaching decrepit age to tread the measures;  ...   56\n",
       "138128        The staring ruffian shall it keep in quiet,   43\n",
       "138129  Pluck down the rich, enrich the poor with trea...   52\n",
       "138130            It shall be raging mad, and silly mild,   41\n",
       "138131    Make the young old, the old become a child. ...   56\n",
       "...                                                   ...  ...\n",
       "138491  Gutenberg-tm eBooks with only a loose network ...   67\n",
       "138492  Project Gutenberg-tm eBooks are often created ...   66\n",
       "138494  the U.S. unless a copyright notice is included...   63\n",
       "138499  how to make donations to the Project Gutenberg...   63\n",
       "138501     our email newsletter to hear about new eBooks.   46\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop end of document (endnotes)\n",
    "df_trunc.tail(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138501"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df_trunc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trunc = df_trunc[df_trunc.index < max(df_trunc.index)-200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words and find length of tokenized vectors\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    return simple_preprocess(x, min_len=1)\n",
    "\n",
    "df_trunc.text = df_trunc.text.apply(tokenize)\n",
    "df_trunc.len = df_trunc.text.apply(get_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>100742.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>8.461099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.076196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 len\n",
       "count  100742.000000\n",
       "mean        8.461099\n",
       "std         2.076196\n",
       "min         3.000000\n",
       "25%         7.000000\n",
       "50%         8.000000\n",
       "75%        10.000000\n",
       "max        19.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[from, fairest, creatures, we, desire, increase]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[that, thereby, beauty, s, rose, might, never,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[but, as, the, riper, should, by, time, decease]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[his, tender, heir, might, bear, his, memory]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[but, thou, contracted, to, thine, own, bright...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  len\n",
       "1   [from, fairest, creatures, we, desire, increase]    6\n",
       "2  [that, thereby, beauty, s, rose, might, never,...    8\n",
       "3   [but, as, the, riper, should, by, time, decease]    8\n",
       "4      [his, tender, heir, might, bear, his, memory]    7\n",
       "5  [but, thou, contracted, to, thine, own, bright...    8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop any lines with token length < 2\n",
    "df_trunc = df_trunc[df_trunc.len > 2]\n",
    "display(df_trunc.describe(), df_trunc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary of Words\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(df_trunc.text)\n",
    "\n",
    "# Save dictionary for future use\n",
    "dictionary.save('shakespear_gensim_dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus (bag of words)\n",
    "# Note the corpus is out of order, so it cannot be used to define sequences later on!\n",
    "corpus = [dictionary.doc2bow(text) for text in df_trunc.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100742"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id['heir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to data file\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "for line in df_trunc.text:\n",
    "    for word in line:\n",
    "        data.append(dictionary.token2id[word])\n",
    "        \n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  2,  0,  5,  1,  4, 12, 13,  6, 11, 10,  8,  9,  7, 15, 14, 20,\n",
       "       18, 19, 16])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "852388"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview data\n",
    "display(data[0:20], len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from fairest creatures we desire increase that thereby beauty s'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check word indexing\n",
    "sentence = \"\"\n",
    "for word in data[0:10]:\n",
    "    sentence += \" \"+dictionary[word]\n",
    "    \n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "txt_data_size = len(data)\n",
    "num_chars = max(data)\n",
    "\n",
    "iteration = 10\n",
    "sequence_length = 10\n",
    "batch_size = round((txt_data_size / sequence_length) + 0.5) # = math.ceil\n",
    "hidden_size = 500  # size of hidden layer of neurons.  \n",
    "learning_rate = 1e-1\n",
    "\n",
    "\n",
    "# model parameters\n",
    "\n",
    "W_xh = np.random.randn(hidden_size, num_chars)*0.01     # weight input -> hidden. \n",
    "W_hh = np.random.randn(hidden_size, hidden_size)*0.01   # weight hidden -> hidden\n",
    "W_hy = np.random.randn(num_chars, hidden_size)*0.01     # weight hidden -> output\n",
    "\n",
    "b_h = np.zeros((hidden_size, 1)) # hidden bias\n",
    "b_y = np.zeros((num_chars, 1)) # output bias\n",
    "\n",
    "h_prev = np.zeros((hidden_size,1)) # h_(t-1)\n",
    "\n",
    "\n",
    "def forwardprop(inputs, targets, h_prev):\n",
    "        \n",
    "    # Since the RNN receives the sequence, the weights are not updated during one sequence.\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} # dictionary\n",
    "    hs[-1] = np.copy(h_prev) # Copy previous hidden state vector to -1 key value.\n",
    "    loss = 0 # loss initialization\n",
    "    \n",
    "    for t in range(len(inputs)): # t is a \"time step\" and is used as a key(dic).  \n",
    "        \n",
    "        xs[t] = np.zeros((num_chars,1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(W_xh, xs[t]) + np.dot(W_hh, hs[t-1]) + b_h) # hidden state.\n",
    "        ys[t] = np.dot(W_hy, hs[t]) + b_y # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars.\n",
    "        \n",
    "        # Softmax. -> The sum of probabilities is 1 even without the exp() function, but all of the elements are positive through the exp() function.\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss). Efficient and simple code\n",
    "\n",
    "#         y_class = np.zeros((num_chars, 1)) \n",
    "#         y_class[targets[t]] =1\n",
    "#         loss += np.sum(y_class*(-np.log(ps[t]))) # softmax (cross-entropy loss)\n",
    "\n",
    "    return loss, ps, hs, xs\n",
    "\n",
    "\n",
    "def backprop(ps, inputs, hs, xs, targets):\n",
    "\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy) # make all zero matrices.\n",
    "    dbh, dby = np.zeros_like(b_h), np.zeros_like(b_y)\n",
    "    dhnext = np.zeros_like(hs[0]) # (hidden_size,1) \n",
    "\n",
    "    # reversed\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t]) # shape (num_chars,1).  \"dy\" means \"dloss/dy\"\n",
    "        dy[targets[t]] -= 1 # backprop into y. After taking the soft max in the input vector, subtract 1 from the value of the element corresponding to the correct label.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy \n",
    "        dh = np.dot(W_hy.T, dy) + dhnext # backprop into h. \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity #tanh'(x) = 1-tanh^2(x)\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(W_hh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]: \n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients.  \n",
    "    \n",
    "    return dWxh, dWhh, dWhy, dbh, dby\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = data[data_pointer:data_pointer+sequence_length]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_pointer = 0\n",
    "num_docs = len(corpus)\n",
    "\n",
    "# memory variables for Adagrad\n",
    "mWxh, mWhh, mWhy = np.zeros_like(W_xh), np.zeros_like(W_hh), np.zeros_like(W_hy)\n",
    "mbh, mby = np.zeros_like(b_h), np.zeros_like(b_y) \n",
    "\n",
    "for i in range(iteration):\n",
    "    h_prev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    data_pointer = 0 # go from start of data\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        \n",
    "        inputs = data[data_pointer:data_pointer+sequence_length]\n",
    "        targets = data[data_pointer+1:data_pointer+sequence_length+1]    # t + 1\n",
    "            \n",
    "        if (data_pointer+sequence_length+1 >= num_docs and b == batch_size-1): # processing of the last part of the input data. \n",
    "#             targets.append(char_to_int[txt_data[0]])   # When the data doesn't fit, add the first char to the back.\n",
    "            targets.append(num_chars+1)   # When the data doesn't fit, add space(\" \") to the back.\n",
    "\n",
    "\n",
    "        # forward\n",
    "        loss, ps, hs, xs = forwardprop(inputs, targets, h_prev)\n",
    "#         print(loss)\n",
    "    \n",
    "        # backward\n",
    "        dWxh, dWhh, dWhy, dbh, dby = backprop(ps, inputs, hs, xs, targets) \n",
    "        \n",
    "        \n",
    "    # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([W_xh, W_hh, W_hy, b_h, b_y], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam # elementwise\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update      \n",
    "    \n",
    "        data_pointer += sequence_length # move data pointer\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        print ('iter %d, loss: %f' % (i, loss)) # print progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate x_train, y_train, x_test, y_test\n",
    "data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y: random lengths from data(X) plus next word in sequence(y)\n",
    "from multiprocessing import Pool\n",
    "\n",
    "sequence_length = 10  # maximum sequence length\n",
    "max_index = len(data)  # maximum index in data (prevent out_of_bounds indexing.  may still happen)\n",
    "num_data_points = 800  # Total number of data points to create\n",
    "num_threads = 8  # Number of processes to spawn\n",
    "\n",
    "\n",
    "def create_training_data(num_data_points):\n",
    "    temp_data = []\n",
    "    for _ in range(num_data_points):\n",
    "        start = np.random.choice(range(max_index))\n",
    "        length = sequence_length\n",
    "        try:\n",
    "            # Attempt to lookup and append sequence\n",
    "            temp_data.append(\n",
    "                data[start:start+length+1]\n",
    "            )\n",
    "        except:\n",
    "            # If fail, step back the sequence length\n",
    "            temp_data.append(\n",
    "                data[start-length:start+1]\n",
    "            )\n",
    "    \n",
    "    return temp_data\n",
    "    \n",
    "\n",
    "# Not exact number of datapoints returned\n",
    "p = Pool(num_threads)\n",
    "observations_per_thread = round(num_data_points/num_threads)\n",
    "worker_list = p.map(create_training_data, [observations_per_thread]*num_threads)\n",
    "\n",
    "# Flatten list of lists returned by mutliprocessing into big dataset\n",
    "dataset = []\n",
    "for work in worker_list:\n",
    "    dataset += work\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2514,    20, 18012,  1196,    99,   135,  3370,   923,   486,\n",
       "           20],\n",
       "       [  175,  5305,   120,  1766,    15,   477,   320,    89,    24,\n",
       "           80],\n",
       "       [   39,    20, 11420, 23409,    99, 23408,   486,    41, 23410,\n",
       "        23411],\n",
       "       [10661,   522,  7749,    20,   896,   338,    68,    20,   154,\n",
       "           99],\n",
       "       [  195,    99,   241,  9101,   951,   477,  1280,  3302,    43,\n",
       "          433]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1451,   58, 3102,   20,   58])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pop_end(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for x in dataset:\n",
    "        X.append(x[0:len(x)-1]) \n",
    "        y.append(x[len(x)-1])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X, y = pop_end(dataset)\n",
    "\n",
    "display(X[0:5], y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset out for future use\n",
    "import pickle\n",
    "\n",
    "with open('shakespeare_seq.pkl', 'wb+') as file:\n",
    "    pickle.dump(dataset, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbrandon/anaconda3/envs/tensorflow/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Try one-hot encoding y\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 84)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array([5.0000e+00, 1.0000e+01, 1.1000e+01, 1.4000e+01, 1.5000e+01,\n",
       "        2.0000e+01, 2.4000e+01, 2.9000e+01, 3.2000e+01, 3.3000e+01,\n",
       "        4.1000e+01, 4.3000e+01, 5.2000e+01, 5.8000e+01, 6.8000e+01,\n",
       "        7.2000e+01, 7.7000e+01, 1.0300e+02, 1.0400e+02, 1.0500e+02,\n",
       "        1.0800e+02, 1.2000e+02, 1.3500e+02, 1.4000e+02, 1.4600e+02,\n",
       "        1.5600e+02, 2.3400e+02, 2.8600e+02, 2.9200e+02, 2.9800e+02,\n",
       "        3.0000e+02, 3.2300e+02, 4.6600e+02, 4.7600e+02, 4.7700e+02,\n",
       "        4.8300e+02, 4.8600e+02, 5.2800e+02, 5.6500e+02, 5.6600e+02,\n",
       "        5.6700e+02, 5.9200e+02, 9.7700e+02, 1.0540e+03, 1.0750e+03,\n",
       "        1.1030e+03, 1.2140e+03, 1.2250e+03, 1.2800e+03, 1.2970e+03,\n",
       "        1.3190e+03, 1.3780e+03, 1.4510e+03, 1.6620e+03, 1.7870e+03,\n",
       "        1.9170e+03, 2.3280e+03, 2.5540e+03, 2.5670e+03, 2.7710e+03,\n",
       "        2.9740e+03, 3.1020e+03, 3.1650e+03, 3.2440e+03, 3.8960e+03,\n",
       "        3.9760e+03, 4.2230e+03, 4.3480e+03, 5.9310e+03, 6.1900e+03,\n",
       "        6.4290e+03, 7.5510e+03, 8.6360e+03, 9.0940e+03, 1.0781e+04,\n",
       "        1.0976e+04, 1.2520e+04, 1.2625e+04, 1.4888e+04, 1.7113e+04,\n",
       "        1.7261e+04, 2.2022e+04, 2.2848e+04, 2.4026e+04])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display y_onehot's shape and respective categories (important for future lookup to reconstruct text)\n",
    "display(y_onehot.shape, encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "maxlen = 10\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Some parameters to play around with\n",
    "batch_size = 32\n",
    "num_classes = y_onehot.shape[1]\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(100000, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train model\n",
      "Train on 640 samples, validate on 160 samples\n",
      "Epoch 1/10\n",
      "640/640 [==============================] - 4s 6ms/sample - loss: 4.4026 - accuracy: 0.2141 - val_loss: 4.3626 - val_accuracy: 0.3250\n",
      "Epoch 2/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 4.2408 - accuracy: 0.2359 - val_loss: 4.0291 - val_accuracy: 0.0625\n",
      "Epoch 3/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 3.5138 - accuracy: 0.2188 - val_loss: 3.1704 - val_accuracy: 0.2875\n",
      "Epoch 4/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 2.4660 - accuracy: 0.4563 - val_loss: 2.1445 - val_accuracy: 0.4750\n",
      "Epoch 5/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 1.4956 - accuracy: 0.7156 - val_loss: 1.2055 - val_accuracy: 0.8375\n",
      "Epoch 6/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 0.8698 - accuracy: 0.8984 - val_loss: 0.6002 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 0.5029 - accuracy: 0.9844 - val_loss: 0.2846 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 0.2972 - accuracy: 0.9969 - val_loss: 0.1410 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 0.2051 - accuracy: 0.9984 - val_loss: 0.0823 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "640/640 [==============================] - 3s 4ms/sample - loss: 0.1476 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4cabedb2d0>"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "print('Train model')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "160/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 776us/sample - loss: -14363.0594 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Score Model\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Outputs, Summary, Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01200438, 0.01179334, 0.01258095, 0.01205203, 0.01077612,\n",
       "        0.01189918, 0.01338654, 0.01233046, 0.01173097, 0.01315766,\n",
       "        0.01238448, 0.01178144, 0.01165742, 0.01278678, 0.01154115,\n",
       "        0.01142499, 0.01335228, 0.01124833, 0.0125392 , 0.01232252,\n",
       "        0.01127132, 0.01144112, 0.01181753, 0.01455888, 0.01224464,\n",
       "        0.01155005, 0.01181156, 0.01167727, 0.01250418, 0.01329488,\n",
       "        0.0118439 , 0.01216083, 0.01281726, 0.01161152, 0.01336071,\n",
       "        0.01307061, 0.013046  , 0.01010635, 0.01228938, 0.01233593,\n",
       "        0.01157697, 0.01148916, 0.0103361 , 0.01229525, 0.0116712 ,\n",
       "        0.01237564, 0.01252307, 0.01126496, 0.01105954, 0.01123849,\n",
       "        0.0129279 , 0.01203072, 0.01112518, 0.01222807, 0.01189715,\n",
       "        0.01167547, 0.01277985, 0.01206879, 0.0117103 , 0.01155843,\n",
       "        0.0121914 , 0.01174473, 0.01167021, 0.01090048, 0.01164963,\n",
       "        0.01089328, 0.0112113 , 0.01103728, 0.01063625, 0.01123132,\n",
       "        0.01258451, 0.01198867, 0.0123234 , 0.01167315, 0.01229706,\n",
       "        0.01176241, 0.01194764, 0.00940468, 0.01030318, 0.01235143,\n",
       "        0.01216785, 0.01125065, 0.01189854, 0.01148463],\n",
       "       [0.01212851, 0.01168958, 0.01179271, 0.01245194, 0.01222076,\n",
       "        0.01299327, 0.01284568, 0.01152036, 0.01151831, 0.01376019,\n",
       "        0.01224008, 0.01157944, 0.01175171, 0.01307425, 0.01154816,\n",
       "        0.01152197, 0.01290909, 0.01148135, 0.01099133, 0.01041462,\n",
       "        0.01202194, 0.01209411, 0.01260821, 0.01298989, 0.01201744,\n",
       "        0.01051867, 0.0121042 , 0.01221661, 0.01208294, 0.01188188,\n",
       "        0.01190778, 0.01182407, 0.01332548, 0.0109824 , 0.01227219,\n",
       "        0.01198857, 0.01156629, 0.01192381, 0.01177103, 0.01228705,\n",
       "        0.01101867, 0.01203216, 0.01195304, 0.0119301 , 0.01190342,\n",
       "        0.0126785 , 0.01160065, 0.01400972, 0.0120797 , 0.0125247 ,\n",
       "        0.01194723, 0.01110483, 0.01140993, 0.01228372, 0.01069466,\n",
       "        0.0120713 , 0.01187071, 0.01101231, 0.01155775, 0.01218126,\n",
       "        0.01158676, 0.01177023, 0.01223553, 0.0114504 , 0.01274759,\n",
       "        0.01072549, 0.01155628, 0.01101391, 0.01186702, 0.01187663,\n",
       "        0.01248589, 0.01249694, 0.01152243, 0.01170789, 0.01163471,\n",
       "        0.01174587, 0.01205041, 0.01089389, 0.01157354, 0.01212684,\n",
       "        0.01196855, 0.01239412, 0.01098362, 0.01090318],\n",
       "       [0.01267376, 0.01208496, 0.01210204, 0.01161668, 0.01050945,\n",
       "        0.01245594, 0.01499567, 0.01157288, 0.01239916, 0.01162811,\n",
       "        0.01281613, 0.01303983, 0.01217921, 0.01324706, 0.01122395,\n",
       "        0.01012145, 0.01234583, 0.01008564, 0.01263179, 0.01217127,\n",
       "        0.01136275, 0.01114241, 0.01201385, 0.01202501, 0.01332284,\n",
       "        0.01056535, 0.01220521, 0.01111044, 0.01182311, 0.01183425,\n",
       "        0.01297095, 0.01177451, 0.01336481, 0.01079147, 0.01295105,\n",
       "        0.01225729, 0.01403688, 0.01137932, 0.01265447, 0.01325634,\n",
       "        0.01279179, 0.01206165, 0.01037454, 0.01148098, 0.0116682 ,\n",
       "        0.01140215, 0.01302863, 0.01141903, 0.01238163, 0.01171025,\n",
       "        0.0136992 , 0.0109798 , 0.01183078, 0.01185436, 0.01247918,\n",
       "        0.01198534, 0.01290874, 0.01227864, 0.01081813, 0.01128033,\n",
       "        0.01166941, 0.01192238, 0.01158435, 0.01178074, 0.01193888,\n",
       "        0.01077132, 0.01266966, 0.01087186, 0.01093379, 0.01056584,\n",
       "        0.0137575 , 0.01186415, 0.01279089, 0.01189517, 0.01139125,\n",
       "        0.01039076, 0.01225248, 0.01026799, 0.01024595, 0.01243414,\n",
       "        0.01208496, 0.0103266 , 0.01033084, 0.0120827 ],\n",
       "       [0.01246076, 0.01089381, 0.01323191, 0.01152954, 0.01136167,\n",
       "        0.01109253, 0.01261673, 0.01291087, 0.01113862, 0.01424952,\n",
       "        0.01166726, 0.01132111, 0.01242179, 0.01381251, 0.01089159,\n",
       "        0.01178803, 0.01239739, 0.0091748 , 0.01257562, 0.01199998,\n",
       "        0.01430992, 0.01302767, 0.01320374, 0.01231444, 0.01376362,\n",
       "        0.00993696, 0.0116428 , 0.01165739, 0.01197145, 0.01117558,\n",
       "        0.01158446, 0.01199415, 0.01238855, 0.01035243, 0.01374007,\n",
       "        0.01344753, 0.01286406, 0.01181934, 0.01301046, 0.01279018,\n",
       "        0.01038168, 0.01416232, 0.01076545, 0.01167527, 0.01007895,\n",
       "        0.01429936, 0.01145785, 0.01201308, 0.01148618, 0.012439  ,\n",
       "        0.01252769, 0.01023075, 0.01071486, 0.0142111 , 0.0107129 ,\n",
       "        0.01160533, 0.01376029, 0.01280005, 0.01023178, 0.01119103,\n",
       "        0.01219828, 0.0116436 , 0.01148271, 0.01148481, 0.01294838,\n",
       "        0.00997396, 0.01273228, 0.01136326, 0.01276663, 0.01001615,\n",
       "        0.01345266, 0.01164672, 0.01150923, 0.01176254, 0.01088804,\n",
       "        0.01054356, 0.01408636, 0.00924633, 0.01155507, 0.01290146,\n",
       "        0.01060875, 0.01150415, 0.00974956, 0.01065984],\n",
       "       [0.01136879, 0.01217515, 0.01069465, 0.01394686, 0.01061594,\n",
       "        0.01198912, 0.01353534, 0.01318947, 0.01258422, 0.01224036,\n",
       "        0.01337858, 0.01176617, 0.01301763, 0.01168107, 0.01174214,\n",
       "        0.01190122, 0.01435444, 0.01159979, 0.01227075, 0.01089523,\n",
       "        0.01184252, 0.01198675, 0.01172368, 0.01218259, 0.01258111,\n",
       "        0.0129887 , 0.01123191, 0.01197863, 0.01195186, 0.01123309,\n",
       "        0.01290005, 0.01570627, 0.01288102, 0.01203175, 0.01282098,\n",
       "        0.00955927, 0.01065703, 0.01188466, 0.01091265, 0.01121646,\n",
       "        0.01125978, 0.0109151 , 0.01366111, 0.01234795, 0.01029173,\n",
       "        0.01137181, 0.01130699, 0.01223777, 0.01484573, 0.0131155 ,\n",
       "        0.01093219, 0.01042476, 0.00894007, 0.01098262, 0.01309618,\n",
       "        0.01096975, 0.01293679, 0.01074728, 0.01105227, 0.01039813,\n",
       "        0.01109273, 0.00974044, 0.01378853, 0.01177375, 0.01485168,\n",
       "        0.00995271, 0.01264611, 0.01131269, 0.01131988, 0.01094142,\n",
       "        0.01200424, 0.01257341, 0.01294235, 0.01126932, 0.01520806,\n",
       "        0.01113879, 0.01184703, 0.01012852, 0.00962177, 0.01332843,\n",
       "        0.01136108, 0.01071612, 0.01137474, 0.01201491],\n",
       "       [0.01280628, 0.01146503, 0.00993386, 0.01330949, 0.01308203,\n",
       "        0.01483524, 0.01255251, 0.00988909, 0.01111096, 0.01175178,\n",
       "        0.01294331, 0.0118633 , 0.01197339, 0.013777  , 0.01132599,\n",
       "        0.01062477, 0.01224954, 0.012449  , 0.00953437, 0.00955427,\n",
       "        0.00997063, 0.01114528, 0.01229639, 0.01180255, 0.01032857,\n",
       "        0.00956734, 0.01210119, 0.01090168, 0.01198671, 0.01199876,\n",
       "        0.01292099, 0.01030961, 0.01264253, 0.01045869, 0.01171132,\n",
       "        0.01203717, 0.0103843 , 0.01287459, 0.01198199, 0.01250591,\n",
       "        0.01131294, 0.0117922 , 0.01242808, 0.01077716, 0.0140359 ,\n",
       "        0.0102556 , 0.01194138, 0.01545479, 0.01218773, 0.01347808,\n",
       "        0.01218642, 0.01203557, 0.0145925 , 0.01165511, 0.01045621,\n",
       "        0.01195391, 0.0098192 , 0.00967672, 0.01160706, 0.01308841,\n",
       "        0.01219392, 0.01222675, 0.01180726, 0.01235096, 0.01167025,\n",
       "        0.01194866, 0.01220421, 0.01275988, 0.01268034, 0.01437966,\n",
       "        0.01187711, 0.01325387, 0.01070972, 0.01214985, 0.0110932 ,\n",
       "        0.01300558, 0.01047844, 0.01502183, 0.01292223, 0.01079129,\n",
       "        0.01288418, 0.01334399, 0.01028676, 0.01026764],\n",
       "       [0.01247615, 0.0125293 , 0.01215013, 0.01164303, 0.01086529,\n",
       "        0.01269112, 0.01535734, 0.01159306, 0.01219021, 0.01142187,\n",
       "        0.01285931, 0.01296265, 0.01177669, 0.01255203, 0.01156738,\n",
       "        0.00982453, 0.01213117, 0.01034973, 0.01222676, 0.01191964,\n",
       "        0.01109168, 0.01104929, 0.01160825, 0.01190811, 0.01293824,\n",
       "        0.01079332, 0.0123353 , 0.01151502, 0.01170549, 0.01206459,\n",
       "        0.0126865 , 0.01163197, 0.01366442, 0.01117802, 0.0126665 ,\n",
       "        0.0122973 , 0.01350251, 0.01159155, 0.01251632, 0.01316212,\n",
       "        0.01324609, 0.01202192, 0.01082809, 0.0113718 , 0.01223993,\n",
       "        0.01147655, 0.01302734, 0.01177779, 0.01250787, 0.01136961,\n",
       "        0.01339456, 0.01086111, 0.01177928, 0.01137608, 0.01247096,\n",
       "        0.01181208, 0.01231287, 0.0119338 , 0.01108475, 0.011888  ,\n",
       "        0.01164249, 0.01178585, 0.01167233, 0.011895  , 0.01183675,\n",
       "        0.01097778, 0.01297291, 0.01085711, 0.01087094, 0.01086062,\n",
       "        0.01338256, 0.01216984, 0.01302543, 0.01187299, 0.01123956,\n",
       "        0.01073038, 0.01215518, 0.01058862, 0.01022283, 0.01203541,\n",
       "        0.01227333, 0.01054827, 0.01026544, 0.01234394],\n",
       "       [0.01352038, 0.01224011, 0.01247627, 0.011288  , 0.01059098,\n",
       "        0.01232439, 0.01600835, 0.01199179, 0.01145456, 0.01168239,\n",
       "        0.01362364, 0.01293074, 0.01180384, 0.01211569, 0.01172068,\n",
       "        0.00968046, 0.0121718 , 0.01017516, 0.01220794, 0.01254729,\n",
       "        0.01130959, 0.0111078 , 0.0116916 , 0.01202007, 0.01360529,\n",
       "        0.01110574, 0.01198461, 0.01190991, 0.01160692, 0.01305229,\n",
       "        0.01245679, 0.01138433, 0.01289024, 0.01052134, 0.01324757,\n",
       "        0.01253809, 0.01329625, 0.01123615, 0.01239287, 0.0130306 ,\n",
       "        0.01315706, 0.01239562, 0.01059844, 0.01149587, 0.01152479,\n",
       "        0.01126856, 0.0130952 , 0.01155031, 0.01216988, 0.01135608,\n",
       "        0.01362929, 0.01094337, 0.01176028, 0.01100081, 0.01230455,\n",
       "        0.01240803, 0.01241019, 0.01209607, 0.01035147, 0.01160524,\n",
       "        0.01138133, 0.01131595, 0.01152708, 0.0116242 , 0.01225   ,\n",
       "        0.01097477, 0.01350963, 0.01030772, 0.01075265, 0.0103467 ,\n",
       "        0.01367434, 0.011512  , 0.013539  , 0.01222264, 0.01118635,\n",
       "        0.0110772 , 0.01244129, 0.01016613, 0.01023433, 0.01213522,\n",
       "        0.01227835, 0.01096553, 0.00990282, 0.01261116],\n",
       "       [0.0158133 , 0.01198764, 0.01325167, 0.01040624, 0.01046661,\n",
       "        0.01204977, 0.01619198, 0.01176581, 0.00991366, 0.01150418,\n",
       "        0.0157929 , 0.01242972, 0.01141889, 0.01112743, 0.01397787,\n",
       "        0.00939552, 0.01258733, 0.01007919, 0.01104205, 0.01174462,\n",
       "        0.01008825, 0.01130284, 0.01148242, 0.01313885, 0.01280781,\n",
       "        0.01107419, 0.01240629, 0.01214361, 0.01112885, 0.01549646,\n",
       "        0.01110352, 0.01024795, 0.01368108, 0.00989288, 0.01339429,\n",
       "        0.01416101, 0.01340949, 0.00983911, 0.01163112, 0.01296132,\n",
       "        0.01559549, 0.01129677, 0.0111327 , 0.01136757, 0.01212195,\n",
       "        0.01061362, 0.01332605, 0.0118648 , 0.01143432, 0.00982145,\n",
       "        0.01325953, 0.01184026, 0.01291586, 0.01124082, 0.01130209,\n",
       "        0.01313258, 0.01139782, 0.0114916 , 0.00931123, 0.01237198,\n",
       "        0.01086397, 0.01138105, 0.01136069, 0.01148281, 0.01124344,\n",
       "        0.01295107, 0.01361598, 0.00903302, 0.01033562, 0.01038093,\n",
       "        0.01345925, 0.0106356 , 0.01580473, 0.01157457, 0.01133962,\n",
       "        0.01209104, 0.01222495, 0.00999518, 0.00950273, 0.01010264,\n",
       "        0.01450786, 0.01164342, 0.01040254, 0.01289105],\n",
       "       [0.01299915, 0.01218973, 0.01153356, 0.01172259, 0.01129398,\n",
       "        0.01227108, 0.01569144, 0.0121505 , 0.01133277, 0.01192572,\n",
       "        0.01387773, 0.0121256 , 0.01149012, 0.01145598, 0.01237634,\n",
       "        0.01030827, 0.01194843, 0.01080485, 0.01187714, 0.01193809,\n",
       "        0.01152459, 0.01156958, 0.01156243, 0.01173536, 0.01324929,\n",
       "        0.01176651, 0.01184833, 0.01220586, 0.01191272, 0.01319469,\n",
       "        0.01172437, 0.01102878, 0.01211693, 0.0118474 , 0.01211673,\n",
       "        0.01245902, 0.01171854, 0.01156434, 0.01173573, 0.01185321,\n",
       "        0.01354065, 0.01236793, 0.01151143, 0.01169101, 0.0117951 ,\n",
       "        0.01153161, 0.01248764, 0.0118701 , 0.01258011, 0.01141058,\n",
       "        0.01257761, 0.01123   , 0.01198193, 0.01111508, 0.01237797,\n",
       "        0.01159936, 0.01173048, 0.01192331, 0.01052382, 0.01176438,\n",
       "        0.01175854, 0.01119601, 0.01161885, 0.01223322, 0.01164747,\n",
       "        0.01163657, 0.01384501, 0.01103137, 0.01104727, 0.01095904,\n",
       "        0.01260017, 0.01145836, 0.01326794, 0.01153774, 0.01113244,\n",
       "        0.01153394, 0.01229608, 0.01099137, 0.01163731, 0.01177797,\n",
       "        0.01252867, 0.01157709, 0.01053552, 0.01249445]], dtype=float32)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnf",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
